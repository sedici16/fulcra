{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sedici16/fulcra/blob/main/Copia_di_fulcra.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Analysis and Processing:**\n",
        "\n",
        "We began our exploration with two datasets. The primary objective was to understand the relationship between various health metrics and blood glucose levels. Upon merging these datasets, we observed a significant amount of missing data. Through descriptive statistics, we gauged the extent of this missing data across various columns.\n",
        "\n",
        "**Glucose Classification and Impact of Exercise:**\n",
        "\n",
        "To lend structure to our analysis, blood glucose values were labeled into three categories using a dedicated algorithm, categorizing them as low, medium, or high. Our initial inclination was to incorporate exercise-related data, despite numerous 'NaN' values, primarily to maximize data volume for the subsequent modeling phase.\n",
        "\n",
        "When this processed data was analyzed, a prominent pattern emerged: exercise appeared to be a dominant factor influencing glucose levels. This finding, while mirroring scientific consensus, emphasized the impact of physical activity on our body's glucose regulation.\n",
        "\n",
        "**Modeling without Exercise Data:**\n",
        "\n",
        "To further dissect the influence of other factors, we crafted a dataset that purposefully excluded exercise metrics. Given the scarcity of available data, the 'apple watch' column was retained, and the steps were capped at the average value of 14. Employing a Random Forest algorithm on this exercise-exempt data, we observed an accuracy metric of approximately 57%.\n",
        "\n",
        "However, beneath this overarching metric lay a skewed picture. The model excelled at identifying high glucose levels but fumbled when tasked with distinguishing between low and medium glucose values. This hinted at a pronounced class imbalance in our dataset.\n",
        "\n",
        "**Recommendations and Considerations:**\n",
        "\n",
        "1. **Data Collection:** Our current dataset is limited. Sourcing data over more extended periods or from more significant cohorts can drastically enhance our analytical depth and the reliability of our findings.\n",
        "   \n",
        "2. **Data Augmentation and Synthetic Data:** Given the evident gaps and imbalances in the data, employing data augmentation techniques could be pivotal. Furthermore, synthetic datasets could be created and explored to bolster our modeling capabilities without waiting for extensive real-world data collection.\n",
        "   \n",
        "3. **Revisiting the Data Processing:** The presence of numerous 'NaN' values underscores the necessity to either source a more comprehensive dataset or adopt techniques to manage such missing values effectively, lest they skew our analysis and predictions.\n",
        "\n"
      ],
      "metadata": {
        "id": "wbxuIXdjZfcj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQn7m1wiN4Bs"
      },
      "outputs": [],
      "source": [
        "!pip install -U fulcra-api"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from fulcra_api.core import FulcraAPI\n",
        "\n",
        "fulcra = FulcraAPI()"
      ],
      "metadata": {
        "id": "EHL3818mOAbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fulcra.authorize()"
      ],
      "metadata": {
        "id": "C3YBDcxQOIWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = fulcra.time_series_grouped(\n",
        "    start_time=\"2023-03-28 04:00:00.000Z\",\n",
        "    end_time=\"2023-06-27 04:00:00.000Z\",\n",
        "    metrics=[\"BloodGlucose\",\n",
        "        \"DietaryProteinConsumed\",\n",
        "        \"DietaryCarbohydratesConsumed\",\n",
        "        \"DietarySugarConsumed\",\n",
        "        \"DietaryFiberConsumed\",\n",
        "        \"MonounsaturatedFatConsumed\",\n",
        "        \"StepCount\",\n",
        "        \"AppleWatchExerciseTime\",\n",
        "        # Add the \"Location\" metric if it has a specific name in the API\n",
        "    ]\n",
        ")\n"
      ],
      "metadata": {
        "id": "JxNHZCtnOjPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#df2 = fulcra.time_series_grouped(\n",
        "#    start_time=\"2023-06-27 04:00:00.000Z\",\n",
        "#    end_time=\"2023-09-27 04:00:00.000Z\",\n",
        "#    metrics=[\"BloodGlucose\",\n",
        "#        \"DietaryProteinConsumed\",\n",
        "#        \"DietaryCarbohydratesConsumed\",\n",
        "#        \"DietarySugarConsumed\",\n",
        "#        \"DietaryFiberConsumed\",\n",
        "#        \"MonounsaturatedFatConsumed\",\n",
        "#        \"StepCount\",\n",
        "#        \"AppleWatchExerciseTime\",\n",
        "#        # Add the \"Location\" metric if it has a specific name in the API\n",
        "#    ],\n",
        "#    sample_rate=1\n",
        "#)"
      ],
      "metadata": {
        "id": "UTu2DJzc09Xq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import pandas as pd\n",
        "#df = pd.concat([df1, df2])"
      ],
      "metadata": {
        "id": "HoNPtuL81Xt6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(10)"
      ],
      "metadata": {
        "id": "3CpYQhZCO6Ip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**summary**\n",
        "1. **Blood Glucose**: 558 readings, averaging at 112.78 mg/dL, ranging from 70 to 193 mg/dL.\n",
        "2. **Blood Glucose Source**: Two main sources, most frequent being '108881'.\n",
        "3. **Dietary Protein**: 262,784 readings, mostly zeros, with a max of 144 grams.\n",
        "4. **Dietary Carbohydrates**: Average of 0.075 grams, peaking at 479 grams.\n",
        "5. **Step Count**: Averaging at 14 steps, with a max of 750 steps.\n",
        "6. **Apple Watch Exercise Time**: 262,846 records, mostly zeros, with a max of 1.\n",
        "\n",
        "Note: Other dietary components also have majority zeros, with some high max values."
      ],
      "metadata": {
        "id": "TxmwV7YfP-_8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Describe the dataset\n",
        "description = df.describe()\n",
        "\n",
        "print(description)"
      ],
      "metadata": {
        "id": "ZhMnHEnqPL35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV into a DataFrame\n",
        "df_foods = pd.read_csv(\"food-logs.csv\")\n",
        "\n",
        "# Drop the specified columns\n",
        "df_foods = df_foods.drop(columns=['Name', 'Meal', 'Quantity', 'Units', 'Deleted', 'Icon'])\n",
        "\n",
        "df_foods['Calories'] = pd.to_numeric(df_foods['Calories'], errors='coerce')\n",
        "df_foods.dropna(subset=['Calories'], inplace=True)\n",
        "\n",
        "# Display the first few rows to verify\n",
        "print(df_foods.head())\n",
        "print(df_foods.dtypes)\n"
      ],
      "metadata": {
        "id": "isBEFdVDQQxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Entries (Rows)**: There are 1,246 recorded food items.\n",
        "- **Date**: The meals span across 160 unique days with the date \"09/03/2023\" having the most entries (16 times).\n",
        "- **Name of Food Item**: There are 220 distinct food names in this data. \"Cacao + Cashew Butter granola\" appears most frequently with 127 occurrences.\n",
        "- **Icon**: Represented by 100 unique icons, with \"Grain\" being the most common (151 times).\n",
        "- **Meal**: Meals are categorized into 5 unique types, with \"Dinner\" being the most frequent category (345 times).\n",
        "- **Quantity**: On average, the quantity consumed is approximately 13.92 units, but this varies widely with a standard deviation of 43.17 units. The smallest quantity recorded is 0.25 units, and the largest is 330 units.\n",
        "\n",
        "In summary, this dataset contains 1,246 entries of food consumption spread over 160 days, with \"Dinner\" being the most common meal and \"Cacao + Cashew Butter granola\" being the most frequently consumed item."
      ],
      "metadata": {
        "id": "rUU-e4MJQ4va"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "description_food1 = df_foods.describe(include='all')\n",
        "\n",
        "print(description_food1)\n"
      ],
      "metadata": {
        "id": "mEYX177rQsuh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking the lenghts of the 2 dataset for merging\n",
        "#they have different lenghts\n",
        "length_df = len(df)\n",
        "length_df_foods = len(df_foods)\n",
        "\n",
        "print(f\"Number of rows in df: {length_df}\")\n",
        "print(f\"Number of rows in df_foods: {length_df_foods}\")"
      ],
      "metadata": {
        "id": "Xx5GG5doRJ_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Merging the data sets with left Merge**\n",
        "\n",
        "A left merge using two dataframes (`df` from \"Context App Data Service\" and `df_foods` from \"food-logs.csv\") will retain all the rows from the left dataframe (`df` in this case) and match them with any corresponding rows from the right dataframe (`df_foods`). If there's no matching date in `df_foods` for a particular date in `df`, the columns from `df_foods` in the merged dataframe will be filled with NaN values.\n",
        "\n",
        "In simpler terms:\n",
        "- You'll keep all dates from `df`.\n",
        "- For dates that also exist in `df_foods`, you'll see data from both `df` and `df_foods` side by side.\n",
        "- For dates in `df` that don't have a match in `df_foods`, the columns specific to `df_foods` will show NaN, indicating missing data.\n",
        "\n",
        "Given that `df_foods` has fewer rows, this means there will be several dates in the merged dataframe where the columns from `df_foods` will have NaN values due to the lack of matching data."
      ],
      "metadata": {
        "id": "G-BuwCqMS_KC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to timezone-naive datetime for df\n",
        "df.index = df.index.tz_localize(None)\n",
        "\n",
        "# Convert the 'Date' column in df_foods to datetime format\n",
        "df_foods['Date'] = pd.to_datetime(df_foods['Date'])\n",
        "\n",
        "# Convert to timezone-naive datetime for df_foods\n",
        "df_foods['Date'] = df_foods['Date'].dt.tz_localize(None)\n",
        "\n",
        "# Perform the merge\n",
        "result_df = pd.merge(df, df_foods, left_index=True, right_on='Date', how='left')\n",
        "\n",
        "# Set the index to 'Date'\n",
        "result_df.set_index('Date', inplace=True)\n",
        "\n",
        "# Display the first few rows\n",
        "result_df.head()\n",
        "\n"
      ],
      "metadata": {
        "id": "NDojSuDURLyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Data Briefing**:\n",
        "\n",
        "1. **Blood Glucose**: 558 measurements taken, with an average level of 112.78.\n",
        "2. **Dietary & Physical Metrics**:\n",
        "   - Dietary data (protein, carbs, sugar, fiber) recorded 263,867 times.\n",
        "   - Average step count is 14, with a max of 750.\n",
        "3. **Food Log Details**:\n",
        "   - 1,242 entries; 220 unique food items.\n",
        "   - Most frequent item: \"Cacao + Cashew Butter granola\" (127 times).\n",
        "   - Nutritional Averages:\n",
        "     - Fats: 14.27g\n",
        "     - Proteins: 13.53g\n",
        "     - Carbs: 29.84g\n",
        "     - Sugars: 7g\n",
        "     - Fiber: 5.28g\n",
        "4. **Other Nutritional Metrics**: Data on saturated fats, cholesterol (up to 558 mg), and sodium (with a high of 7,200 mg) is also available.\n",
        "\n",
        "---\n",
        "\n",
        "The dataset provides a comprehensive view of dietary patterns, with a detailed breakdown of food items and their nutritional components."
      ],
      "metadata": {
        "id": "fneJyam_UDSq"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l5Lx8eNHUM0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "description2 = result_df.describe(include='all')\n",
        "\n",
        "print (description2)\n"
      ],
      "metadata": {
        "id": "FXXPq9DwTeB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Data Type Conversion**:\n",
        "    The `blood_glucose` column from the `result_df` DataFrame is converted to the `float` data type. This is crucial because subsequent mathematical operations and comparisons will require the data to be in a numerical format.\n",
        "\n",
        "2. **Handling Missing Values**:\n",
        "    Any missing values in the `blood_glucose` column are interpolated. Interpolation is a method of estimating values between two known values. In the context of a DataFrame, it often fills missing data points based on neighboring values, providing a more consistent dataset.\n",
        "\n",
        "3. **Average Glucose Computation**:\n",
        "    After interpolation, the average (mean) of the `blood_glucose` column is recomputed to obtain the `avg_glucose` value.\n",
        "\n",
        "4. **Glucose Labeling Function**:\n",
        "    The `label_glucose_level` function is defined to categorize blood glucose levels. The classification is based on the computed average glucose:\n",
        "   - **Low**: Any value less than the average glucose minus 10 is labeled as \"Low.\"\n",
        "   - **Normal**: Values that lie within a range of 10% above or below the average glucose are labeled as \"Normal.\"\n",
        "   - **High**: Any other value (that doesn't fall into the previous two categories) is labeled as \"High.\"\n",
        "\n",
        "5. **Applying the Labeling Function**:\n",
        "    The `apply` method, combined with a lambda function, is used to apply the `label_glucose_level` function to each row in the `blood_glucose` column. This results in a new column, `new_glucose_label`, which contains the categorized labels.\n",
        "\n",
        "6. **Count of Each Label**:\n",
        "    Finally, the code displays the counts of each label (Low, Normal, High) present in the `new_glucose_label` column using the `value_counts` method. This provides a quick overview of the distribution of glucose levels in the dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "CVvmNpaFaZBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Convert blood_glucose column to float dtype\n",
        "result_df['blood_glucose'] = result_df['blood_glucose'].astype(float)\n",
        "\n",
        "# Interpolate missing values of blood_glucose\n",
        "result_df['blood_glucose'] = result_df['blood_glucose'].interpolate()\n",
        "\n",
        "# Recompute the average glucose after interpolation\n",
        "avg_glucose = result_df['blood_glucose'].mean()\n",
        "\n",
        "print ('average glucose', avg_glucose)\n",
        "\n",
        "def label_glucose_level(value, avg_glucose):#this labels the glucose level\n",
        "    if value < (avg_glucose - 10):\n",
        "        return \"Low\"\n",
        "    elif (avg_glucose - 0.10 * avg_glucose) <= value <= (avg_glucose + 0.10 * avg_glucose):\n",
        "        return \"Normal\"\n",
        "    else:\n",
        "        return \"High\"\n",
        "\n",
        "# Create a new glucose label column based on the updated blood glucose values\n",
        "result_df['new_glucose_label'] = result_df['blood_glucose'].apply(lambda x: label_glucose_level(x, avg_glucose))\n",
        "\n",
        "label_counts = result_df['new_glucose_label'].value_counts()\n",
        "print(label_counts)\n"
      ],
      "metadata": {
        "id": "FXW3JemUUZmz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_df"
      ],
      "metadata": {
        "id": "3eGbwLfjdi1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This Models incluses the exercise data, I have done beacuse of the amount of NaN data in the data set, so I did not want to reduce the data set too much.\n",
        "\n",
        "The code segment below shows: the process of preparing the `result_df` data for machine learning, training a Random Forest Classifier, evaluating its performance, and analyzing the importance of various features in determining glucose levels.\n",
        "\n",
        "Here's a breakdown:\n",
        "\n",
        "1. **Importing Necessary Libraries**:\n",
        "    - Essential libraries and modules like `RandomForestClassifier`, `train_test_split`, `accuracy_score`, and `LabelEncoder` are imported from the scikit-learn library.\n",
        "\n",
        "2. **Data Preparation**:\n",
        "    - The target variable, `new_glucose_label`, is encoded into numerical values using `LabelEncoder` for modeling.\n",
        "    - The encoded target variable is stored in `y`, and then the associated original column, alongside other specified columns, is dropped from the DataFrame to prepare the feature matrix `X`.\n",
        "  \n",
        "3. **Data Splitting**:\n",
        "    - The data is split into training and test sets with an 80-20 ratio using the `train_test_split` function. A random state of 42 is set for reproducibility.\n",
        "\n",
        "4. **Handling Missing Values**:\n",
        "    - Any missing values in the training and test datasets are replaced with the median of the respective columns using the `fillna` method. Using the median is a robust method, especially when outliers are present.\n",
        "\n",
        "5. **Model Training**:\n",
        "    - A Random Forest Classifier is trained on the training data. Again, a random state of 42 ensures the results are reproducible.\n",
        "\n",
        "6. **Evaluation**:\n",
        "    - The trained classifier is used to make predictions on the test set. The accuracy of these predictions is then calculated, with the model achieving an accuracy of approximately 56.46%.\n",
        "\n",
        "7. **Feature Importance Analysis**:\n",
        "    - The importance of each feature in predicting the target variable is extracted from the Random Forest model. The features are then sorted in descending order based on their importance.\n",
        "    - From the listed features, it is evident that dietary aspects like carbohydrates consumed, protein consumed, and dietary fiber are the top three influencers on glucose levels, according to this model. At the bottom, `monounsaturated_fat` has zero importance, meaning it did not play a role in the model's decisions.\n",
        "\n",
        "**Findings and Comments**:\n",
        "- The Random Forest model achieves a moderate accuracy of approximately 56.46%. There might be room for improvement with more advanced preprocessing, feature engineering, or hyperparameter tuning.\n",
        "- Dietary habits seem to significantly influence glucose levels. Specifically, the amount of carbohydrates, protein, and fiber consumed appear to be major factors.\n",
        "- It's interesting to note that both `dietary_carbohydrates_consumed` and `Carbohydrates (g)` are in the top features. This might indicate redundancy in the data, and there could be potential multicollinearity.\n",
        "- On the contrary, the `monounsaturated_fat` feature seems to have no impact on the glucose level prediction, at least within the context of this model.\n",
        "\n",
        "In conclusion, while the model offers insights into potential dietary factors influencing glucose levels, further refinement and data analysis might enhance its predictive capabilities."
      ],
      "metadata": {
        "id": "v0MEDI-qa6qh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Encoding target variable\n",
        "le = LabelEncoder()\n",
        "result_df['new_glucose_label'] = le.fit_transform(result_df['new_glucose_label'])\n",
        "\n",
        "# Define target variable y before dropping columns\n",
        "y = result_df['new_glucose_label']\n",
        "\n",
        "# Drop columns from result_df\n",
        "result_df = result_df.drop(columns=['blood_glucose', 'new_glucose_label','step_count', 'blood_glucose_src', 'apple_watch_exercise_time'])\n",
        "X = result_df.copy()\n",
        "\n",
        "# Splitting the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  # added random state for reproducibility\n",
        "\n",
        "# Handling missing values using median\n",
        "X_train = X_train.fillna(X_train.median(numeric_only=True))\n",
        "X_test = X_test.fillna(X_train.median(numeric_only=True))\n",
        "\n",
        "# Training Random Forest\n",
        "clf = RandomForestClassifier(random_state=42)  # added random state for reproducibility\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predictions and Evaluation\n",
        "y_pred = clf.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "# Feature Importance\n",
        "features = X.columns\n",
        "importances = clf.feature_importances_\n",
        "sorted_importances = sorted(zip(features, importances), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "for feature, importance in sorted_importances:\n",
        "    print(f\"{feature}: {importance}\")\n"
      ],
      "metadata": {
        "id": "OqQIuGPvWz8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "\n",
        "# Confusion matrix\n",
        "conf_mat = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_mat, annot=True, cmap='Blues', fmt='g')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Precision, Recall, F1-score\n",
        "print(classification_report(y_test, y_pred, target_names=le.classes_))\n"
      ],
      "metadata": {
        "id": "0cHMHRhHXStH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classification Metrics:\n",
        "\n",
        "High:\n",
        "\n",
        "Precision: 0.57 suggests that when the model predicts \"High\", it's correct about 57% of the time.\n",
        "Recall: 0.00 means that out of all actual \"High\" instances, the model hardly recognizes any of them. This is concerning.\n",
        "F1-score: 0.00 is a harmonic mean of precision and recall, and this low value indicates a poor performance for the \"High\" class.\n",
        "Low:\n",
        "\n",
        "Precision: 0.30 suggests a low reliability in its \"Low\" predictions.\n",
        "Recall: 0.00 indicates it almost never correctly identifies actual \"Low\" instances.\n",
        "F1-score: 0.00 again indicates poor performance for the \"Low\" class.\n",
        "Normal:\n",
        "\n",
        "Precision: 0.56 means when the model predicts \"Normal\", it's correct 56% of the time.\n",
        "Recall: 1.00 suggests the model identifies almost all \"Normal\" instances correctly.\n",
        "F1-score: 0.72 is relatively good compared to other classes."
      ],
      "metadata": {
        "id": "lcgJ_d-tYhky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Unzipping the feature names and their importance values from your RandomForest model results\n",
        "features, importance_values = zip(*sorted_importances)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.barh(features, importance_values, align='center', color='skyblue')  # Added color for aesthetics\n",
        "plt.xlabel('Importance')\n",
        "plt.ylabel('Features')\n",
        "plt.title('Feature Importance using RandomForest')\n",
        "plt.gca().invert_yaxis()  # Highest importance at the top\n",
        "plt.tight_layout()  # Adjust layout for better visualization\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "jnPAHlJyY2qQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The feature importance chart reveals which attributes or variables most influence the prediction of the glucose label classes. In essence, these importances signify how significant each feature is in making accurate predictions.\n",
        "\n",
        "For instance:\n",
        "\n",
        "dietary_carbohydrates_consumed: This feature has the highest importance, indicating that the amount of carbohydrates consumed plays a crucial role in predicting the glucose label.\n",
        "\n",
        "dietary_protein_consumed: Protein consumption also holds significant weight in predictions, being the second most crucial feature.\n",
        "\n",
        "dietary_fiber_consumed and dietary_sugar_consumed: Dietary fibers and sugars also impact glucose levels. These features, being the third and fourth most influential, respectively, suggest that their intake levels are pivotal for predicting blood glucose conditions.\n",
        "\n",
        "On the other hand, monounsaturated_fat has an importance of 0.0, meaning it did not play any discernible role in the model's predictions.\n",
        "\n",
        "The model's accuracy stands at approximately 56.45%. This indicates that the model correctly predicts the glucose label for about 56.45% of the test instances. Given the features available and the complexity of predicting glucose levels, there's scope for further model optimization or exploration of additional relevant features."
      ],
      "metadata": {
        "id": "xZuxiEK7ZQJX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Xboost**\n",
        "\n",
        "The code implements the XGBoost algorithm to predict glucose levels. After preparing and encoding the data, unnecessary columns are removed. The data is then split into training and test sets, with missing values filled using the median. An XGBoost classifier is trained on the dataset, achieving an accuracy that's printed out. Finally, the importance of each feature used in prediction is ranked and displayed. The switch to XGBoost represents an exploration to potentially achieve better accuracy compared to previous models."
      ],
      "metadata": {
        "id": "_5vPmlzyb0tz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to timezone-naive datetime for df\n",
        "df.index = df.index.tz_localize(None)\n",
        "\n",
        "# Convert the 'Date' column in df_foods to datetime format\n",
        "df_foods['Date'] = pd.to_datetime(df_foods['Date'])\n",
        "\n",
        "# Convert to timezone-naive datetime for df_foods\n",
        "df_foods['Date'] = df_foods['Date'].dt.tz_localize(None)\n",
        "\n",
        "# Perform the merge\n",
        "result_df = pd.merge(df, df_foods, left_index=True, right_on='Date', how='left')\n",
        "\n",
        "# Set the index to 'Date'\n",
        "result_df.set_index('Date', inplace=True)\n",
        "\n",
        "\n",
        "#####################\n",
        "\n",
        "# Convert blood_glucose column to float dtype\n",
        "result_df['blood_glucose'] = result_df['blood_glucose'].astype(float)\n",
        "\n",
        "# Interpolate missing values of blood_glucose\n",
        "result_df['blood_glucose'] = result_df['blood_glucose'].interpolate()\n",
        "\n",
        "# Recompute the average glucose after interpolation\n",
        "avg_glucose = result_df['blood_glucose'].mean()\n",
        "\n",
        "#print ('average glucose', avg_glucose)\n",
        "\n",
        "def label_glucose_level(value, avg_glucose):#this labels the glucose level\n",
        "    if value < (avg_glucose - 10):\n",
        "        return \"Low\"\n",
        "    elif (avg_glucose - 0.10 * avg_glucose) <= value <= (avg_glucose + 0.10 * avg_glucose):\n",
        "        return \"Normal\"\n",
        "    else:\n",
        "        return \"High\"\n",
        "\n",
        "# Create a new glucose label column based on the updated blood glucose values\n",
        "result_df['new_glucose_label'] = result_df['blood_glucose'].apply(lambda x: label_glucose_level(x, avg_glucose))\n",
        "\n",
        "label_counts = result_df['new_glucose_label'].value_counts()\n",
        "\n",
        "\n",
        "##############################\n",
        "\n",
        "\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Assuming result_df is already loaded and processed as per your provided code\n",
        "\n",
        "# Encoding target variable\n",
        "le = LabelEncoder()\n",
        "if 'new_glucose_label' in result_df.columns:\n",
        "    result_df['new_glucose_label'] = le.fit_transform(result_df['new_glucose_label'])\n",
        "\n",
        "# Define target variable y before dropping columns\n",
        "y = result_df['new_glucose_label']\n",
        "\n",
        "# Drop columns from result_df\n",
        "columns_to_drop = ['blood_glucose', 'new_glucose_label', 'blood_glucose_src', 'step_count', 'apple_watch_exercise_time']\n",
        "for col in columns_to_drop:\n",
        "    if col in result_df.columns:\n",
        "        result_df = result_df.drop(columns=col)\n",
        "\n",
        "X = result_df.copy()\n",
        "\n",
        "# Splitting the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Handling missing values using median\n",
        "X_train = X_train.fillna(X_train.median(numeric_only=True))\n",
        "X_test = X_test.fillna(X_train.median(numeric_only=True))\n",
        "\n",
        "# Training XGBoost\n",
        "clf = xgb.XGBClassifier(random_state=42, use_label_encoder=False)\n",
        "clf.fit(X_train, y_train, eval_metric='logloss')  # Specified eval_metric to suppress warning\n",
        "\n",
        "# Predictions and Evaluation\n",
        "y_pred = clf.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "# Feature Importance\n",
        "features = X.columns\n",
        "importances = clf.feature_importances_\n",
        "sorted_importances = sorted(zip(features, importances), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "for feature, importance in sorted_importances:\n",
        "    print(f\"{feature}: {importance}\")\n"
      ],
      "metadata": {
        "id": "fccRcm1Ik31s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to timezone-naive datetime for df\n",
        "df.index = df.index.tz_localize(None)\n",
        "\n",
        "# Convert the 'Date' column in df_foods to datetime format\n",
        "df_foods['Date'] = pd.to_datetime(df_foods['Date'])\n",
        "\n",
        "# Convert to timezone-naive datetime for df_foods\n",
        "df_foods['Date'] = df_foods['Date'].dt.tz_localize(None)\n",
        "\n",
        "# Perform the merge\n",
        "result_df = pd.merge(df, df_foods, left_index=True, right_on='Date', how='left')\n",
        "\n",
        "# Set the index to 'Date'\n",
        "result_df.set_index('Date', inplace=True)\n",
        "\n",
        "\n",
        "#####################\n",
        "\n",
        "# Convert blood_glucose column to float dtype\n",
        "result_df['blood_glucose'] = result_df['blood_glucose'].astype(float)\n",
        "\n",
        "# Interpolate missing values of blood_glucose\n",
        "result_df['blood_glucose'] = result_df['blood_glucose'].interpolate()\n",
        "\n",
        "# Recompute the average glucose after interpolation\n",
        "avg_glucose = result_df['blood_glucose'].mean()\n",
        "\n",
        "#print ('average glucose', avg_glucose)\n",
        "\n",
        "def label_glucose_level(value, avg_glucose):#this labels the glucose level\n",
        "    if value < (avg_glucose - 10):\n",
        "        return \"Low\"\n",
        "    elif (avg_glucose - 0.10 * avg_glucose) <= value <= (avg_glucose + 0.10 * avg_glucose):\n",
        "        return \"Normal\"\n",
        "    else:\n",
        "        return \"High\"\n",
        "\n",
        "# Create a new glucose label column based on the updated blood glucose values\n",
        "result_df['new_glucose_label'] = result_df['blood_glucose'].apply(lambda x: label_glucose_level(x, avg_glucose))\n",
        "\n",
        "label_counts = result_df['new_glucose_label'].value_counts()\n",
        "\n",
        "\n",
        "##############################\n",
        "\n",
        "\n",
        "\n",
        "# Print out 'step_count' column\n",
        "print(result_df['step_count'])\n",
        "\n",
        "# Print out the first 5 rows of 'step_count'\n",
        "print(result_df['step_count'].head())\n",
        "\n",
        "# Describe 'step_count' column\n",
        "print(result_df['step_count'].describe())\n"
      ],
      "metadata": {
        "id": "S5pMJdDQLnAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Filtered out Exercise**\n",
        "\n",
        "The script begins by defining a function `filter_steps` which is responsible for filtering the original dataframe based on the `step_count` and the value of `apple_watch_exercise_time`. It then drops the columns used for filtering and returns the filtered dataframe. The average steps in the dataset are 14. Using this function, the data is narrowed down to observations with steps ranging from 0 to 14.\n",
        "\n",
        "Subsequently, the `no_exercise` dataframe, which now represents a subset of the data without exercise influence, is processed and split into training and test sets. The target variable (`new_glucose_label`) is encoded.\n",
        "\n",
        "A Random Forest classifier is then trained on this `no_exercise` data. After training, predictions are made on the test set, and the accuracy is printed. The classifier's feature importances are also extracted and displayed, helping identify which features are most influential in predicting glucose levels in the absence of exercise."
      ],
      "metadata": {
        "id": "Ar2mGlrzcQLD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_steps(df, n_steps, include_watch=1):\n",
        "    \"\"\"\n",
        "    Filters the dataframe to only include rows where 'step_count' is less than or equal to n_steps\n",
        "    and, based on the user's choice, where 'apple_watch_exercise_time' is not equal to 1. It then drops these columns.\n",
        "\n",
        "    Parameters:\n",
        "    - df: the dataframe to be filtered\n",
        "    - n_steps: the maximum step count to be included\n",
        "    - include_watch: if set to 0, rows where 'apple_watch_exercise_time' is equal to 1 are excluded; if set to 1, they are included\n",
        "\n",
        "    Returns:\n",
        "    - no_exercise_df: a dataframe with filtered rows\n",
        "    \"\"\"\n",
        "\n",
        "    if include_watch == 0:\n",
        "        no_exercise_df = df[(df['step_count'] <= n_steps) & (df['apple_watch_exercise_time'] != 1)]\n",
        "    else:\n",
        "        no_exercise_df = df[df['step_count'] <= n_steps]\n",
        "\n",
        "    # Drop the 'step_count' and 'apple_watch_exercise_time' columns\n",
        "    no_exercise_df = no_exercise_df.drop(['step_count', 'apple_watch_exercise_time'], axis=1)\n",
        "\n",
        "    # Describe the resulting dataframe\n",
        "    print(no_exercise_df.describe())\n",
        "\n",
        "    return no_exercise_df\n",
        "\n",
        "# Usage\n",
        "# If you want to exclude rows where apple_watch_exercise_time is 1\n",
        "#no_exercise = filter_steps(result_df, 10, 0)\n",
        "\n",
        "# If you want to include all rows regardless of the value of apple_watch_exercise_time\n",
        "#mean steps is 14, better leave the apple wath data included\n",
        "no_exercise = filter_steps(result_df, 14, 1)#the steps will work from zero to 14, then the data becomes too thin to make a model\n"
      ],
      "metadata": {
        "id": "9Sa-2esJN0wb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# 2. Split the no_exercise DataFrame\n",
        "le = LabelEncoder()\n",
        "no_exercise['new_glucose_label'] = le.fit_transform(no_exercise['new_glucose_label'])\n",
        "y_no_exercise = no_exercise['new_glucose_label']\n",
        "X_no_exercise = no_exercise.drop(columns=['blood_glucose', 'new_glucose_label', 'blood_glucose_src']) # 'dietary_protein_consumed', 'blood_glucose_src', 'dietary_carbohydrates_consumed', 'dietary_sugar_consumed','dietary_fiber_consumed']\n",
        "\n",
        "# 3. Follow the pipeline\n",
        "\n",
        "# Splitting the dataset\n",
        "X_train_no_exercise, X_test_no_exercise, y_train_no_exercise, y_test_no_exercise = train_test_split(X_no_exercise, y_no_exercise, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# Handling missing values using median\n",
        "X_train_no_exercise = X_train_no_exercise.fillna(X_train_no_exercise.median(numeric_only=True))\n",
        "X_test_no_exercise = X_test_no_exercise.fillna(X_train_no_exercise.median(numeric_only=True))\n",
        "\n",
        "# Training Random Forest on no_exercise data\n",
        "clf_no_exercise = RandomForestClassifier(random_state=42)\n",
        "clf_no_exercise.fit(X_train_no_exercise, y_train_no_exercise)\n",
        "\n",
        "# Predictions and Evaluation\n",
        "y_pred_no_exercise = clf_no_exercise.predict(X_test_no_exercise)\n",
        "print(\"Accuracy on no_exercise data:\", accuracy_score(y_test_no_exercise, y_pred_no_exercise))\n",
        "\n",
        "# Feature Importance for no_exercise data\n",
        "features_no_exercise = X_no_exercise.columns\n",
        "importances_no_exercise = clf_no_exercise.feature_importances_\n",
        "sorted_importances_no_exercise = sorted(zip(features_no_exercise, importances_no_exercise), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "for feature, importance in sorted_importances_no_exercise:\n",
        "    print(f\"{feature}: {importance}\")"
      ],
      "metadata": {
        "id": "dKVb-V0lOQHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def predict_glucose_level(features_input):\n",
        "    \"\"\"\n",
        "    Predicts the glucose level given a set of features.\n",
        "\n",
        "    Parameters:\n",
        "    - features_input (dict): A dictionary where the keys are the feature names and the values are the corresponding feature values.\n",
        "\n",
        "    Returns:\n",
        "    - str: The predicted glucose level label ('High', 'Normal', or 'Low').\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert the input dictionary to a DataFrame\n",
        "    input_df = pd.DataFrame([features_input])\n",
        "\n",
        "    # Reorder columns of input_df to match the training data\n",
        "    input_df = input_df[X_train_no_exercise.columns]\n",
        "\n",
        "    # Handle missing values if any using the median values from X_train_no_exercise\n",
        "    input_df = input_df.fillna(X_train_no_exercise.median(numeric_only=True))\n",
        "\n",
        "    # Make a prediction\n",
        "    prediction = clf_no_exercise.predict(input_df)\n",
        "\n",
        "    # Convert the prediction to its string representation\n",
        "    label_map = {\n",
        "        0: \"Low\",\n",
        "        1: \"Medium\",\n",
        "        2: \"High\"\n",
        "    }\n",
        "\n",
        "    label = label_map[prediction[0]]\n",
        "\n",
        "    return label\n",
        "\n",
        "# Example usage:\n",
        "features_input = {\n",
        "    'dietary_protein_consumed': 5,  # Approximated from apple\n",
        "    'Calories': 35,  # Approximated from apple\n",
        "    'Protein (g)': 0.5,  # Approximated from apple\n",
        "    'dietary_carbohydrates_consumed': 5,  # Approximated from apple\n",
        "    'dietary_fiber_consumed': 4,  # Approximated from apple\n",
        "    'dietary_sugar_consumed': 1,  # Approximated from apple\n",
        "    'Fat (g)': 0.3,  # Approximated from apple\n",
        "    'Sodium (mg)': 2,  # Approximated from apple\n",
        "    'Saturated Fat (g)': 0.1,  # Almost negligible, but we'll include it\n",
        "    'Sugars (g)': 1,  # Approximated from apple\n",
        "    'Fiber (g)': 4,  # Approximated from apple\n",
        "    'Cholesterol (mg)': 0,  # Apples don't contain cholesterol\n",
        "    'monounsaturated_fat': 0.1,  # Estimate; not typically specified for apples but they have a small amount of various fats\n",
        "    'Carbohydrates (g)': 1  # Approximated from apple\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "predicted_label = predict_glucose_level(features_input)\n",
        "print(f\"The predicted glucose level is: {predicted_label}\")\n"
      ],
      "metadata": {
        "id": "t6hPE6DGafJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation on Model's Performance Issues\n",
        "\n",
        "The model's inability to generate accurate predictions can be attributed to a couple of significant issues:\n",
        "\n",
        "Data Imbalance: A prominent challenge arises from the imbalance in the dataset. When one class (e.g., 'High') is over-represented compared to others, the model tends to be biased towards predicting that over-represented class. As a result, even when presented with feature values that should lead to different class predictions, the model might still predict the dominant class because it has \"learned\" to do so from the skewed training data. This data imbalance makes it difficult for the model to learn the nuanced patterns that differentiate between classes, especially for under-represented ones.\n",
        "\n",
        "Redundant Features: Additionally, there are repeated or very similar features in the dataset, such as dietary_protein_consumed and Protein (g). Without a clear distinction between what each of these features represents, it introduces redundancy in the feature space. This can lead to multicollinearity issues, where two or more variables are highly correlated, causing inefficiencies and potential overfitting. Redundant features can confuse the model and make it harder to ascertain which features are truly important for predictions.\n",
        "\n",
        "In summary, for the model to predict more effectively, it's essential to address the data imbalance and clarify or eliminate redundant features. It would involve reprocessing the data and potentially retraining the model using refined datasets."
      ],
      "metadata": {
        "id": "Bn4HIcLteVnC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Plotting Feature Importance\n",
        "features, importance_values = zip(*sorted_importances_no_exercise)  # Unzipping the feature names and their importance values\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.barh(features, importance_values, align='center')\n",
        "plt.xlabel('Importance')\n",
        "plt.title('Feature Importance for no_exercise data')\n",
        "plt.gca().invert_yaxis()  # Highest importance at the top\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "aqbvZELNVWIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "\n",
        "# 2. Plotting Confusion Matrix\n",
        "matrix = confusion_matrix(y_test_no_exercise, y_pred_no_exercise)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(matrix, annot=True, cmap='Blues', fmt='g')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix for no_exercise data')\n",
        "plt.show()\n",
        "\n",
        "# Printing classification report which includes precision, recall, and F1-score\n",
        "print(classification_report(y_test_no_exercise, y_pred_no_exercise))\n"
      ],
      "metadata": {
        "id": "Xd_bMsFMVZRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary of Model Performance and Recommendations:\n",
        "\n",
        "The model's performance evaluation indicates a significant skew towards predicting the \"High blood glucose\" class (Class 2). While it exhibits a commendable prediction accuracy for this class, it falls short in effectively predicting the other two classes. This behavior highlights a classic case of class imbalance in the dataset, where the over-represented class tends to dominate the model's predictions.\n",
        "\n",
        "Addressing class imbalance is critical as relying on a model with such skew can lead to misleading results and undermine the objective of having a balanced and nuanced prediction across all classes. Potential solutions include data-level interventions like oversampling the minority class or undersampling the majority class, using synthetic data generation techniques.\n",
        "\n",
        "Data Collection: The organization should invest in gathering more data, especially for the underrepresented classes. This would not only address the class imbalance but also reduce the impact of missing values, leading to a more robust dataset.\n",
        "\n",
        "In conclusion, to improve the model's utility and reliability, addressing the class imbalance is of paramount importance. This will ensure that the predictions are well-rounded and representative of all classes, providing more actionable insights."
      ],
      "metadata": {
        "id": "kc7ld1yHW1UM"
      }
    }
  ]
}